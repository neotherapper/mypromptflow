# Universal Quality Assessment Framework
# System-wide quality scoring for all content types across all platforms
# Location: @meta/unified-intelligence/quality-engine/universal-quality-framework.yaml

metadata:
  version: "1.0.0"
  purpose: "Universal quality assessment across all platforms and content types"
  scope: "YouTube, GitHub, Reddit, HackerNews, Official Sources"
  last_updated: "2025-07-31"

# UNIVERSAL QUALITY DIMENSIONS
quality_dimensions:
  source_authority:
    weight: 0.25
    description: "Credibility and expertise of the source"
    scoring_criteria:
      official_sources: 1.0
      expert_creators: 0.8-0.9
      community_sources: 0.6-0.8
      unknown_sources: 0.3-0.5
    
    platform_specific:
      youtube:
        indicators: ["subscriber count", "channel age", "consistent uploads", "community engagement"]
        thresholds:
          high_authority: ">100k subscribers, >2 years, weekly uploads"
          medium_authority: ">10k subscribers, >1 year, monthly uploads"
          low_authority: "<10k subscribers, irregular uploads"
      
      github:
        indicators: ["star count", "fork count", "contributor quality", "maintenance activity"]
        thresholds:
          high_authority: ">10k stars, active maintenance, quality contributors"
          medium_authority: ">1k stars, regular updates, community involvement"
          low_authority: "<1k stars, sporadic updates"
      
      reddit:
        indicators: ["subreddit quality", "upvote ratio", "comment quality", "moderator standards"]
        thresholds:
          high_authority: "Major programming subreddits, >90% upvote ratio"
          medium_authority: "Specialized subreddits, >75% upvote ratio"
          low_authority: "Small communities, <75% upvote ratio"

  content_accuracy:
    weight: 0.30
    description: "Factual correctness and technical accuracy"
    scoring_criteria:
      verified_information: 1.0
      expert_validated: 0.9
      community_validated: 0.7
      unverified: 0.4
    
    validation_methods:
      cross_platform_consistency: "Same information across multiple reliable sources"
      official_source_verification: "Confirmation from official documentation"
      expert_consensus: "Agreement among recognized experts"
      community_feedback: "Positive reception from knowledgeable community"
    
    accuracy_indicators:
      positive: ["code examples work", "references official docs", "explains concepts clearly"]
      negative: ["broken examples", "outdated information", "misleading claims"]

  relevance_alignment:
    weight: 0.20
    description: "Alignment with user interests and topic scope"
    scoring_criteria:
      perfect_match: 1.0
      strong_relevance: 0.8
      moderate_relevance: 0.6
      weak_relevance: 0.3
    
    relevance_factors:
      topic_keywords: "Match with specified topic keywords"
      user_preferences: "Alignment with user's stated preferences"
      contextual_relevance: "Relevance within current context or project"
      timeliness: "Recency and current applicability"

  completeness_depth:
    weight: 0.15
    description: "Thoroughness and comprehensiveness of coverage"
    scoring_criteria:
      comprehensive: 1.0
      substantial: 0.8
      adequate: 0.6
      superficial: 0.3
    
    depth_indicators:
      coverage_breadth: "Covers multiple aspects of the topic"
      detail_level: "Provides sufficient implementation details"
      practical_examples: "Includes working code examples or demonstrations"
      context_explanation: "Explains why and when to use approaches"

  constitutional_compliance:
    weight: 0.10
    description: "Adherence to ethical guidelines and quality standards"
    scoring_criteria:
      fully_compliant: 1.0
      mostly_compliant: 0.8
      partially_compliant: 0.5
      non_compliant: 0.0
    
    compliance_checks:
      truthfulness: "Information is factually accurate and verifiable"
      fairness: "Balanced perspective without harmful bias"
      transparency: "Clear about limitations and sources"
      harm_prevention: "Avoids content that could cause harm"

# PLATFORM-SPECIFIC QUALITY ADAPTATIONS
platform_adaptations:
  youtube:
    quality_adjustments:
      transcript_coherence: "Bonus for clear, well-structured speech"
      visual_aids: "Bonus for effective visual demonstrations"
      engagement_quality: "High-quality audience engagement in comments"
      educational_value: "Clear learning outcomes and practical applications"
    
    penalty_factors:
      clickbait_title: "-0.1 for misleading titles"
      poor_audio: "-0.1 for unclear audio quality"
      outdated_content: "-0.2 for seriously outdated information"
      promotional_heavy: "-0.1 for excessive self-promotion"

  github:
    quality_adjustments:
      documentation_quality: "Bonus for comprehensive README and docs"
      code_quality: "Bonus for clean, well-commented code"
      active_maintenance: "Bonus for regular updates and issue responses"
      community_health: "Bonus for welcoming community and good practices"
    
    penalty_factors:
      poor_documentation: "-0.2 for inadequate documentation"
      inactive_project: "-0.3 for abandoned projects"
      security_issues: "-0.5 for known security vulnerabilities"
      license_issues: "-0.2 for unclear or problematic licensing"

  official_sources:
    quality_adjustments:
      official_designation: "+0.3 bonus for verified official status"
      content_freshness: "Bonus for regularly updated content"
      comprehensive_coverage: "Bonus for thorough topic coverage"
      clear_authorship: "Bonus for clear author credentials"
    
    penalty_factors:
      outdated_content: "-0.3 for significantly outdated information"
      incomplete_coverage: "-0.1 for gaps in important topics"

# USER PREFERENCE INTEGRATION
user_preference_integration:
  preference_multipliers:
    range: [0.5, 2.0]
    application: "Multiply base quality score by user preference factor"
    
  preference_sources:
    stated_preferences:
      weight: 0.6
      source: "user-preferences.json stated_preferences"
    
    learned_preferences:
      weight: 0.3
      source: "user-preferences.json learned_preferences"
    
    engagement_patterns:
      weight: 0.1
      source: "user-preferences.json engagement_tracking"
  
  preference_calculation:
    channel_preference: "Direct preference for specific channels/sources"
    topic_preference: "Preference for content topics"
    format_preference: "Preference for content formats (tutorial, discussion, etc.)"
    depth_preference: "Preference for technical depth level"

# SCORING ALGORITHM
scoring_algorithm:
  base_calculation:
    formula: "(source_authority * 0.25) + (content_accuracy * 0.30) + (relevance_alignment * 0.20) + (completeness_depth * 0.15) + (constitutional_compliance * 0.10)"
    range: [0.0, 1.0]
  
  preference_adjustment:
    formula: "base_score * user_preference_multiplier"
    constraints: "Final score capped at 1.0"
  
  platform_adjustment:
    formula: "preference_adjusted_score + platform_bonuses - platform_penalties"
    constraints: "Final score clamped to [0.0, 1.0] range"

# QUALITY THRESHOLDS
quality_thresholds:
  auto_approve: 0.90
  high_quality: 0.80
  medium_quality: 0.60
  review_required: 0.40
  auto_reject: 0.30
  
  threshold_actions:
    auto_approve: "Include in high-priority digest, save to knowledge vault"
    high_quality: "Include in daily digest, save to knowledge vault"
    medium_quality: "Include in weekly digest, conditional save"
    review_required: "Flag for manual review before inclusion"
    auto_reject: "Exclude from all digests, log for pattern analysis"

# ADAPTIVE QUALITY LEARNING
adaptive_learning:
  feedback_integration:
    user_feedback: "Like/dislike affects future scoring for similar content"
    engagement_feedback: "Time spent, actions taken affect content type preferences"
    source_feedback: "Source promotion/demotion affects authority scores"
  
  learning_rules:
    positive_feedback: "Increase quality scores for similar content by 0.05"
    negative_feedback: "Decrease quality scores for similar content by 0.05"
    source_promotion: "Increase source authority by 0.1"
    source_demotion: "Decrease source authority by 0.1"
  
  decay_mechanism:
    learning_decay: "Learned adjustments decay by 5% weekly unless reinforced"
    recency_bias: "More recent feedback weighted higher (90% for current week)"

# CROSS-PLATFORM VALIDATION
cross_platform_validation:
  consistency_checks:
    information_alignment: "Verify consistent information across platforms"
    source_credibility: "Cross-validate source authority across platforms"
    community_consensus: "Check for community agreement on quality"
  
  validation_bonuses:
    cross_platform_consistency: "+0.05 for information confirmed across platforms"
    expert_endorsement: "+0.1 for content endorsed by recognized experts"
    community_validation: "+0.05 for positive community reception"
  
  validation_penalties:
    contradictory_information: "-0.2 for information contradicted elsewhere"
    disputed_claims: "-0.1 for claims disputed by credible sources"
    community_rejection: "-0.1 for content widely criticized by community"

# QUALITY REPORTING
quality_reporting:
  score_breakdown:
    components: "Show contribution of each quality dimension"
    adjustments: "Show user preference and platform adjustments"
    final_score: "Display final quality score with explanation"
  
  improvement_suggestions:
    low_authority: "Suggest seeking additional authoritative sources"
    low_accuracy: "Suggest fact-checking or verification steps"
    low_relevance: "Suggest refining topic focus or search criteria"
    low_completeness: "Suggest seeking more comprehensive coverage"
  
  trend_analysis:
    source_performance: "Track quality trends for individual sources"
    topic_coverage: "Analyze quality coverage across different topics"
    user_satisfaction: "Monitor correlation between quality scores and user engagement"

# IMPLEMENTATION GUIDELINES
implementation_guidelines:
  quality_assessment_frequency:
    real_time: "Assess quality for all new content as it's discovered"
    batch_processing: "Re-assess quality for existing content weekly"
    user_triggered: "Re-assess quality when user provides feedback"
  
  performance_optimization:
    caching: "Cache quality scores for stable content"
    incremental_updates: "Update only changed quality factors"
    parallel_processing: "Assess multiple content items simultaneously"
  
  error_handling:
    missing_data: "Use default values for missing quality indicators"
    api_failures: "Graceful degradation when external validation fails"
    timeout_handling: "Set reasonable timeouts for quality assessment operations"