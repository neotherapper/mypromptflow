# MCP Server Multi-Dimensional Scoring System
# Objective, reproducible scoring for 1,150+ MCP servers

metadata:
  version: "1.0.0"
  created: "2025-07-21"
  purpose: "Standardized scoring for MCP server prioritization"
  total_weight: 1.0

# Six-dimensional scoring system
scoring_dimensions:
  information_retrieval_relevance:
    weight: 0.25
    scale: "0-10"
    description: "How directly the server supports information processing tasks"
    scoring_guide:
      10: "Core information processing (search, extraction, analysis)"
      8-9: "Direct information support (databases, content access)"
      6-7: "Information workflow support (processing, transformation)"
      4-5: "Indirect information value (monitoring, communication)"
      2-3: "Minimal information relevance (utilities, development tools)"
      0-1: "No information processing relevance"
    examples:
      score_10: ["Fetch (web content)", "Memory (knowledge graphs)", "Search engines"]
      score_8: ["Databases", "Vector search", "Document processing"]
      score_6: ["File systems", "Cloud storage", "Content transformation"]
      score_4: ["Monitoring tools", "Communication platforms"]
      score_2: ["Development utilities", "System administration"]
      score_0: ["Games", "Entertainment", "Non-information tools"]

  setup_complexity:
    weight: 0.20
    scale: "0-10 (10=simple, 1=complex)"
    description: "Ease of installation, configuration, and deployment"
    reverse_scoring: true  # Higher score = less complex
    scoring_guide:
      10: "No dependencies, immediate use, single command setup"
      8-9: "Minimal dependencies, simple configuration, quick setup"
      6-7: "Moderate dependencies, some configuration required"
      4-5: "Multiple dependencies, complex configuration"
      2-3: "Extensive dependencies, difficult setup, expert knowledge required"
      0-1: "Extremely complex, multiple systems, enterprise-level setup"
    dependency_impact:
      no_dependencies: 10
      single_simple_dependency: 8-9
      multiple_simple_dependencies: 6-7
      complex_dependencies: 4-5
      enterprise_dependencies: 2-3
      extreme_complexity: 0-1

  maintenance_status:
    weight: 0.20
    scale: "0-10"
    description: "Active development, maintenance quality, and long-term viability"
    scoring_guide:
      10: "Official provider maintained, active development, recent commits"
      8-9: "Well-maintained, regular updates, responsive to issues"
      6-7: "Stable maintenance, periodic updates, some community support"
      4-5: "Minimal maintenance, infrequent updates, limited support"
      2-3: "Poor maintenance, outdated, minimal support"
      0-1: "Abandoned, archived, no maintenance"
    provider_categories:
      official_anthropic: 10  # Anthropic reference servers
      official_vendor: 9-10   # Redis, Qdrant, etc.
      enterprise_backed: 8-9  # AWS, Google, Microsoft
      active_community: 6-8   # Well-maintained community projects
      individual_maintained: 4-7  # Personal projects, variable quality
      archived_abandoned: 0-2  # No longer maintained

  documentation_quality:
    weight: 0.15
    scale: "0-10"
    description: "Documentation completeness, clarity, and usability"
    scoring_guide:
      10: "Comprehensive docs, examples, tutorials, API reference, troubleshooting"
      8-9: "Good documentation, examples, clear setup instructions"
      6-7: "Basic documentation, setup guide, some examples"
      4-5: "Minimal documentation, basic setup information"
      2-3: "Poor documentation, unclear instructions, missing information"
      0-1: "No documentation, README only, or completely unclear"
    documentation_components:
      setup_guide: 2  # Points for clear setup instructions
      api_reference: 2  # Points for complete API documentation
      examples: 2  # Points for working examples
      troubleshooting: 2  # Points for troubleshooting guide
      tutorials: 1  # Points for step-by-step tutorials
      architecture: 1  # Points for architecture/design documentation

  community_adoption:
    weight: 0.10
    scale: "0-10"
    description: "Community usage, popularity, and ecosystem integration"
    scoring_guide:
      10: "Widely adopted, large community, ecosystem integration"
      8-9: "Good adoption, active community, some ecosystem presence"
      6-7: "Moderate adoption, some community activity"
      4-5: "Limited adoption, small community"
      2-3: "Minimal adoption, little community"
      0-1: "No adoption, no community, new/unknown"
    metrics:
      github_stars:
        1000_plus: 3
        500_999: 2
        100_499: 1
        under_100: 0
      github_forks:
        100_plus: 2
        50_99: 1
        under_50: 0
      ecosystem_presence:
        multiple_integrations: 3
        some_integrations: 2
        few_integrations: 1
        no_integrations: 0
      community_activity:
        very_active: 2
        moderately_active: 1
        minimal_activity: 0

  integration_potential:
    weight: 0.10
    scale: "0-10"
    description: "API quality, ease of integration, and multi-server coordination"
    scoring_guide:
      10: "Excellent API design, easy integration, plays well with others"
      8-9: "Good API, straightforward integration, some coordination features"
      6-7: "Decent API, moderate integration effort"
      4-5: "Basic API, integration challenges, limited coordination"
      2-3: "Poor API, difficult integration, incompatibility issues"
      0-1: "No API, impossible to integrate, or completely incompatible"
    api_quality_factors:
      json_rpc_compliance: 2  # MCP protocol compliance
      error_handling: 2  # Proper error reporting and handling
      documentation: 2  # API documentation quality
      consistency: 2  # Consistent API patterns
      extensibility: 1  # Support for extensions
      performance: 1  # API performance characteristics

# Composite scoring calculation
composite_scoring:
  formula: "weighted_sum"
  calculation: |
    composite_score = (
      (info_retrieval_relevance * 0.25) +
      (setup_complexity * 0.20) +
      (maintenance_status * 0.20) +
      (documentation_quality * 0.15) +
      (community_adoption * 0.10) +
      (integration_potential * 0.10)
    )
  
  rounding: "two_decimal_places"
  scale: "0.0-10.0"

# Tier classification based on composite scores
tier_classification:
  tier_1_immediate:
    threshold: 8.0
    description: "Top priority for immediate implementation"
    target_count: "10-15 servers"
    implementation_timeline: "Next 30 days"
    characteristics:
      - "High information retrieval value"
      - "Reasonable setup complexity"
      - "Active maintenance"
      - "Good documentation"
  
  tier_2_medium_term:
    threshold: 6.0
    threshold_max: 7.9
    description: "Good for enhanced capabilities"
    target_count: "20-25 servers"
    implementation_timeline: "Next 60-90 days"
    characteristics:
      - "Good information value"
      - "Acceptable complexity"
      - "Stable maintenance"
      - "Basic documentation"
  
  tier_3_future:
    threshold: 0.0
    threshold_max: 5.9
    description: "Future considerations or specialized use"
    target_count: "Remaining servers"
    implementation_timeline: "As needed"
    characteristics:
      - "Specialized or limited value"
      - "Higher complexity or maintenance concerns"
      - "Experimental or niche applications"

# Quality assurance for scoring
quality_assurance:
  scorer_guidelines:
    - "Score based on objective criteria, not personal preference"
    - "Use examples and benchmarks for consistency"
    - "Document reasoning for all scores"
    - "Review scores for internal consistency"
    - "Consider multiple perspectives for complex servers"
  
  validation_checks:
    - "Composite scores within expected ranges (0.0-10.0)"
    - "Tier classification matches expected distributions"
    - "High-priority servers have justifiable scores"
    - "Similar servers have consistent scoring patterns"
    - "Score components add up correctly"

# Bias mitigation
bias_mitigation:
  known_biases:
    - "Preference for familiar technologies"
    - "Overvaluing official/enterprise solutions"
    - "Undervaluing specialized tools"
    - "Complexity bias (favoring simple over powerful)"
  
  mitigation_strategies:
    - "Use objective criteria with specific examples"
    - "Include diverse perspectives in scoring"
    - "Regular calibration with example servers"
    - "Peer review of controversial scores"
    - "Documentation of scoring rationale"

# Example scoring templates
example_scoring:
  high_priority_server:
    name: "Example High-Priority Server"
    information_retrieval_relevance: 9  # Core information processing
    setup_complexity: 8  # Moderate dependencies
    maintenance_status: 10  # Official maintenance
    documentation_quality: 9  # Excellent docs
    community_adoption: 7  # Good adoption
    integration_potential: 9  # Great API
    composite_score: 8.75
    tier: "tier_1_immediate"
  
  medium_priority_server:
    name: "Example Medium-Priority Server"
    information_retrieval_relevance: 7  # Supporting information tasks
    setup_complexity: 6  # More complex setup
    maintenance_status: 7  # Community maintained
    documentation_quality: 6  # Basic documentation
    community_adoption: 5  # Limited adoption
    integration_potential: 7  # Good API
    composite_score: 6.55
    tier: "tier_2_medium_term"
  
  low_priority_server:
    name: "Example Low-Priority Server"
    information_retrieval_relevance: 4  # Indirect information value
    setup_complexity: 4  # Complex setup
    maintenance_status: 5  # Minimal maintenance
    documentation_quality: 4  # Poor documentation
    community_adoption: 3  # Minimal adoption
    integration_potential: 5  # Basic API
    composite_score: 4.25
    tier: "tier_3_future"