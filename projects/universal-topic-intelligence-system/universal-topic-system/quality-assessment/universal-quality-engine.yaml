# Universal Quality Assessment Engine
# Topic-agnostic quality scoring and validation system for multi-topic monitoring

engine_metadata:
  name: "Universal Quality Assessment Engine"
  version: "1.0.0"
  description: "Configurable quality assessment system that adapts to any topic domain"
  author: "AI Knowledge Intelligence Orchestrator"
  created_date: "2025-07-27"
  framework_compatibility: "Universal Topic Monitoring System v1.0"

# Core Quality Dimensions - Universal across all topics
core_quality_dimensions:
  
  source_authority:
    weight: 0.25
    description: "Credibility and expertise of information source"
    measurement_criteria:
      domain_authority: "Official designation, institutional recognition, expert status"
      track_record: "Historical accuracy, consistency, reliability"
      community_recognition: "Citations, references, peer acknowledgment"
      technical_expertise: "Domain knowledge, experience, qualifications"
    scoring_algorithm:
      base_score: "Topic-specific authority mapping"
      modifiers: ["+0.2 for verified official sources", "+0.1 for expert recognition", "-0.3 for anonymity", "-0.5 for poor track record"]
      minimum_threshold: 0.3
      maximum_score: 1.0
      
  content_accuracy:
    weight: 0.30
    description: "Factual correctness and verifiability of information"
    measurement_criteria:
      fact_verification: "Cross-reference validation against authoritative sources"
      claim_substantiation: "Evidence provided for significant claims"
      error_detection: "Identification of factual inaccuracies or misstatements"
      update_currency: "Timeliness and relevance of information"
    scoring_algorithm:
      verification_multiplier: 1.0  # Full score for verified facts
      unverified_penalty: 0.3       # 30% reduction for unverified claims
      error_penalty: 0.5            # 50% reduction for detected errors
      staleness_decay: 0.1          # 10% reduction per month for outdated info
      
  relevance_alignment:
    weight: 0.20
    description: "How well content matches topic scope and significance"
    measurement_criteria:
      keyword_matching: "Presence of topic-specific primary and secondary keywords"
      context_alignment: "Content focus matches topic boundaries and interests"
      significance_level: "Importance of development within topic domain"
      noise_filtering: "Absence of irrelevant or off-topic information"
    scoring_algorithm:
      keyword_base: "Weighted keyword matching score from topic configuration"
      context_multiplier: [1.2, 1.0, 0.8, 0.6]  # High, Medium, Low, Irrelevant context
      significance_boost: [0.3, 0.1, 0.0]       # High, Medium, Low significance
      noise_penalty: 0.4                        # 40% reduction for off-topic content
      
  completeness_depth:
    weight: 0.15
    description: "Thoroughness and comprehensiveness of information coverage"
    measurement_criteria:
      coverage_scope: "How completely the content addresses the topic"
      detail_level: "Depth of analysis and explanation provided"
      context_provision: "Background information and supporting details"
      actionability: "Practical utility and implementable insights"
    scoring_algorithm:
      scope_coverage: [1.0, 0.8, 0.6, 0.4]      # Complete, Good, Partial, Minimal
      detail_multiplier: [1.2, 1.0, 0.8]        # Deep, Adequate, Surface level
      context_bonus: 0.1                        # 10% bonus for good context
      actionability_bonus: 0.15                 # 15% bonus for practical value
      
  constitutional_compliance:
    weight: 0.10
    description: "Adherence to ethical guidelines and constitutional AI principles"
    measurement_criteria:
      factual_integrity: "Commitment to truth and accuracy"
      bias_minimization: "Balanced perspective and fairness"
      harm_prevention: "Avoidance of potentially harmful content"
      transparency: "Clear acknowledgment of limitations and uncertainties"
    scoring_algorithm:
      integrity_base: 1.0                       # Start with full score
      bias_penalty: 0.3                        # 30% reduction for detected bias
      harm_penalty: 0.8                        # 80% reduction for potential harm
      transparency_bonus: 0.1                  # 10% bonus for acknowledged uncertainty

# Topic-Specific Configuration Framework
topic_adaptation_framework:
  
  quality_threshold_scaling:
    description: "Adapt quality thresholds based on topic characteristics"
    scaling_factors:
      volatility_adjustment:
        high_volatility: 0.9    # Crypto, stocks - lower thresholds due to speculation
        medium_volatility: 1.0  # AI/ML, tech - standard thresholds
        low_volatility: 1.1     # Academic, government - higher thresholds
        
      authority_distribution:
        centralized_authority: 1.2    # Clear official sources - higher authority weight
        distributed_authority: 1.0    # Mixed authority - standard weight
        community_driven: 0.8         # Community-led - lower authority requirement
        
      information_density:
        high_density: 1.1      # Fast-moving topics - bonus for coverage
        medium_density: 1.0    # Standard coverage expectations
        low_density: 0.9       # Slow-moving topics - accept lower volume
        
  domain_specific_indicators:
    description: "Topic-specific quality indicators and scoring modifications"
    indicator_categories:
      
      technical_topics:
        additional_criteria: ["mathematical_accuracy", "reproducibility", "peer_review"]
        quality_modifiers:
          peer_reviewed: +0.2    # Bonus for academic validation
          code_availability: +0.1  # Bonus for implementation details
          mathematical_errors: -0.5  # Heavy penalty for technical errors
          
      market_topics:
        additional_criteria: ["data_currency", "source_transparency", "conflict_disclosure"]
        quality_modifiers:
          real_time_data: +0.15   # Bonus for current market information
          source_bias: -0.3       # Penalty for undisclosed financial interests
          speculation_flag: -0.2  # Penalty for unsubstantiated predictions
          
      regulatory_topics:
        additional_criteria: ["official_validation", "legal_accuracy", "implementation_timeline"]
        quality_modifiers:
          official_source: +0.25  # High bonus for government/regulatory sources
          legal_errors: -0.6      # Heavy penalty for legal misstatements
          implementation_detail: +0.1  # Bonus for practical guidance
          
      research_topics:
        additional_criteria: ["methodology_soundness", "sample_size", "statistical_significance"]
        quality_modifiers:
          methodology_description: +0.15  # Bonus for clear methodology
          sample_inadequacy: -0.4         # Penalty for insufficient data
          significance_testing: +0.1      # Bonus for statistical rigor

# Multi-Agent Quality Validation System
multi_agent_validation:
  
  validation_levels:
    level_1_worker_validation:
      scope: "Content extraction and basic quality checks"
      responsibilities:
        - "Source accessibility and content extraction accuracy"
        - "Basic format validation and parsing correctness"
        - "Preliminary relevance filtering based on keywords"
        - "Flag obvious quality issues for escalation"
      quality_focus: ["extraction_accuracy", "format_compliance", "basic_relevance"]
      escalation_triggers: ["extraction_failure", "format_corruption", "irrelevance_threshold"]
      
    level_2_specialist_validation:
      scope: "Domain-specific quality assessment and content analysis"
      responsibilities:
        - "Topic-specific relevance and significance scoring"
        - "Content accuracy validation using domain knowledge"
        - "Source authority assessment within topic context"
        - "Cross-reference validation against known information"
      quality_focus: ["domain_relevance", "factual_accuracy", "source_credibility"]
      escalation_triggers: ["accuracy_disputes", "authority_conflicts", "significance_disagreement"]
      
    level_3_architect_validation:
      scope: "Strategic quality oversight and cross-topic coordination"
      responsibilities:
        - "Quality threshold calibration for topic characteristics"
        - "Cross-topic quality consistency validation"
        - "Resource allocation optimization based on quality patterns"
        - "Quality framework adaptation and improvement"
      quality_focus: ["consistency_across_topics", "threshold_optimization", "pattern_recognition"]
      escalation_triggers: ["systemic_quality_issues", "framework_limitations", "optimization_opportunities"]
      
    level_4_queen_validation:
      scope: "Universal quality governance and constitutional compliance"
      responsibilities:
        - "Constitutional AI principle enforcement across all topics"
        - "Universal quality standard maintenance and evolution"
        - "Crisis response for quality failures or ethical concerns"
        - "Quality framework strategic direction and governance"
      quality_focus: ["constitutional_compliance", "ethical_standards", "universal_consistency"]
      authority: "Final quality decisions and framework modifications"

# Cross-Topic Quality Intelligence
cross_topic_intelligence:
  
  pattern_recognition:
    shared_quality_patterns:
      description: "Quality patterns that apply across multiple topics"
      examples:
        - "Official source reliability patterns"
        - "Community discussion quality indicators"
        - "Speculation vs fact identification patterns"
        - "Bias detection across different domains"
      learning_mechanism: "Continuous pattern analysis across all monitored topics"
      
    topic_specific_adaptations:
      description: "Quality adaptations learned for specific topic domains"
      examples:
        - "Crypto-specific speculation filtering"
        - "AI/ML technical accuracy validation"
        - "Regulatory compliance verification methods"
        - "Research methodology assessment criteria"
      adaptation_frequency: "Weekly pattern analysis and threshold updates"
      
  quality_knowledge_sharing:
    source_reputation_database:
      description: "Shared database of source quality assessments across topics"
      tracking_metrics: ["accuracy_history", "bias_patterns", "authority_changes", "reliability_trends"]
      update_frequency: "Real-time with weekly aggregation analysis"
      
    validation_method_library:
      description: "Reusable validation methods and quality check procedures"
      categories: ["fact_checking", "bias_detection", "authority_verification", "relevance_assessment"]
      sharing_mechanism: "Successful methods automatically available to all topics"

# Quality Assessment Pipeline
assessment_pipeline:
  
  stage_1_initial_scoring:
    inputs: ["raw_content", "source_metadata", "topic_configuration"]
    processing:
      - "Apply topic-specific keyword weighting for relevance scoring"
      - "Calculate source authority score using topic authority mapping"
      - "Perform initial accuracy checks against known information"
      - "Assess content completeness using topic-specific criteria"
    outputs: ["initial_quality_scores", "flagged_issues", "validation_requirements"]
    
  stage_2_specialist_analysis:
    inputs: ["initial_scores", "content_analysis", "domain_context"]
    processing:
      - "Domain-specific accuracy validation by specialist agents"
      - "Significance assessment within topic context"
      - "Cross-reference validation against authoritative sources"
      - "Bias detection using topic-specific indicators"
    outputs: ["refined_quality_scores", "validation_results", "improvement_recommendations"]
    
  stage_3_cross_validation:
    inputs: ["specialist_scores", "validation_results", "quality_history"]
    processing:
      - "Multi-agent consensus building for disputed assessments"
      - "Historical pattern analysis for consistency validation"
      - "Cross-topic quality comparison and normalization"
      - "Constitutional AI compliance verification"
    outputs: ["final_quality_scores", "consensus_metrics", "compliance_validation"]
    
  stage_4_continuous_improvement:
    inputs: ["final_scores", "usage_feedback", "outcome_tracking"]
    processing:
      - "Quality prediction accuracy analysis"
      - "Threshold optimization based on performance metrics"
      - "Pattern learning from quality assessment outcomes"
      - "Framework adaptation for improved accuracy"
    outputs: ["updated_thresholds", "improved_patterns", "framework_optimizations"]

# Performance Monitoring and Optimization
performance_framework:
  
  quality_metrics_tracking:
    accuracy_metrics:
      prediction_accuracy: "How well quality scores predict actual content value"
      false_positive_rate: "Percentage of high-quality content incorrectly flagged as low-quality"
      false_negative_rate: "Percentage of low-quality content incorrectly rated as high-quality"
      target_thresholds: {"accuracy": 0.90, "false_positive": 0.10, "false_negative": 0.05}
      
    consistency_metrics:
      inter_agent_agreement: "Consistency of quality assessments across different agents"
      temporal_consistency: "Stability of quality assessments for similar content over time"
      cross_topic_consistency: "Comparable quality standards applied across different topics"
      target_thresholds: {"agreement": 0.85, "temporal": 0.90, "cross_topic": 0.80}
      
  optimization_mechanisms:
    adaptive_threshold_adjustment:
      frequency: "Weekly analysis with real-time micro-adjustments"
      criteria: "Performance metric trends and quality outcome feedback"
      bounds: "Maximum 10% adjustment per week to maintain stability"
      
    pattern_learning_integration:
      mechanism: "Continuous learning from quality assessment outcomes"
      feedback_loop: "Quality predictions validated against actual content utility"
      pattern_updates: "Automatic integration of successful quality indicators"
      
    resource_allocation_optimization:
      quality_based_prioritization: "Allocate more resources to higher-quality sources"
      efficiency_optimization: "Reduce processing for consistently low-quality sources"
      dynamic_adjustment: "Real-time resource reallocation based on quality patterns"

# Implementation Integration Points
integration_points:
  
  topic_configuration_integration:
    configuration_binding: "Quality engine automatically adapts to topic configuration parameters"
    threshold_inheritance: "Quality thresholds inherit from topic-specific settings"
    indicator_customization: "Quality indicators customize based on topic characteristics"
    
  agent_framework_integration:
    worker_agent_interface: "Standardized quality check APIs for worker agents"
    specialist_validation: "Quality validation protocols integrated into specialist workflows"
    architect_oversight: "Quality governance integrated into architect coordination patterns"
    queen_governance: "Constitutional compliance integrated into queen-level governance"
    
  cross_topic_coordination:
    shared_intelligence: "Quality insights shared across topics for improved accuracy"
    resource_optimization: "Quality-based resource allocation across topic monitoring"
    pattern_recognition: "Cross-topic pattern learning for enhanced quality assessment"
    
  storage_and_reporting:
    quality_metadata: "Quality scores stored with all processed content"
    performance_analytics: "Quality performance tracked and analyzed continuously"
    improvement_tracking: "Quality improvements documented and measured over time"

# Quality Assurance Protocols
quality_assurance:
  
  validation_procedures:
    framework_validation: "Regular validation of quality assessment framework accuracy"
    calibration_validation: "Periodic calibration of quality thresholds against outcomes"
    cross_topic_validation: "Validation of quality consistency across different topics"
    
  error_handling:
    quality_assessment_failures: "Graceful handling of quality assessment errors"
    escalation_procedures: "Clear escalation paths for disputed quality assessments"
    recovery_mechanisms: "Automatic recovery from quality assessment system failures"
    
  continuous_improvement:
    feedback_integration: "User and system feedback integrated into quality improvements"
    performance_optimization: "Continuous optimization based on quality performance metrics"
    framework_evolution: "Regular evolution of quality framework based on learnings"

# Success Metrics and Validation
success_metrics:
  
  operational_metrics:
    assessment_accuracy: "≥90% accuracy in quality predictions"
    processing_efficiency: "≤2 seconds average assessment time per content item"
    consistency_rate: "≥85% inter-agent agreement on quality assessments"
    
  quality_outcomes:
    content_utility: "≥95% of high-scored content proves valuable to users"
    noise_reduction: "≥80% reduction in low-quality content reaching users"
    discovery_enhancement: "≥90% of significant developments correctly identified as high-quality"
    
  system_integration:
    framework_adoption: "100% successful integration across all monitored topics"
    cross_topic_optimization: "≥20% efficiency improvement through shared quality intelligence"
    adaptive_improvement: "≥5% quality assessment accuracy improvement per month"

This Universal Quality Assessment Engine provides the foundation for maintaining high-quality information processing across all topics while adapting to the unique characteristics and requirements of each domain.